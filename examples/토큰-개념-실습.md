# 토큰 개념 실습 가이드

이 문서는 `../doc/토큰을 모르면 AI는 늘 신기한 상자로 남아요.md`에서 설명하는 개념들을 실제로 체험할 수 있는 실습 가이드입니다.

---

## 실습 목표

이 실습을 통해 다음 개념들을 직접 경험합니다:

1. **토큰화**: 텍스트를 조각으로 나누는 과정
2. **Bigram 학습**: "자주 붙는 쌍을 센다"는 것의 의미
3. **다음 토큰 예측**: 문장 생성이 실제로 어떻게 일어나는지
4. **Bigram vs Trigram**: 문맥 길이가 왜 중요한지
5. **Usage (비용)**: 왜 토큰 수가 비용의 기준인지
6. **확률적 선택**: 왜 같은 질문에 다른 답이 나올 수 있는지
7. **희소성 문제**: "처음 보는 문맥"이 왜 문제인지

---

## 1. 토큰화 체험

### 실습 1-1: 토큰 개수 세기

```bash
# CLI를 사용하여 토큰화
./gradlew :mini-ai-cli:run --args="tokenize '오늘은 날씨가 좋다'"
```

**관찰 포인트**:
- "오늘은 날씨가 좋다" → 몇 개의 토큰으로 나뉘나요?
- 띄어쓰기가 기준이 되나요?

### 실습 1-2: 다양한 문장으로 토큰 수 비교

```bash
./gradlew :mini-ai-cli:run --args="tokenize '할 수 있다'"
./gradlew :mini-ai-cli:run --args="tokenize '할수있다'"
./gradlew :mini-ai-cli:run --args="tokenize '할 수 있을까'"
```

**교육 포인트**:
- 띄어쓰기가 있으면: 3개 토큰
- 띄어쓰기가 없으면: 1개 토큰
- 이것이 **"AI가 보는 조각의 단위"**입니다

**비용 관점**:
```
"할 수 있다" = 3토큰 = 비용 3배
"할수있다" = 1토큰 = 비용 1배
```

---

## 2. Bigram 학습 체험: "세기"의 의미

### 실습 2-1: 간단한 코퍼스로 학습

```bash
# 교육용 데이터로 학습
./gradlew :mini-ai-cli:run --args="train --corpus data/교육용-토큰개념.txt --output data/교육-bigram.json"
```

**학습 후 확인**:
```bash
# 생성된 artifact 확인
cat data/교육-bigram.json | head -50
```

**관찰 포인트**:
- `counts` 필드를 보세요
- 예: `"오늘은" → "날씨가": 2, "기분이": 2`
- 이것이 **"자주 붙는 쌍을 센 표"**입니다

### 실습 2-2: 카운트가 어떻게 확률이 되는지

```
오늘은 → 날씨가 (2번)
오늘은 → 기분이 (2번)
----------------
오늘은 다음에:
  - 날씨가 올 확률 = 2/4 = 50%
  - 기분이 올 확률 = 2/4 = 50%
```

**교육 포인트**:
- 학습 = 카운팅
- 확률 = 카운트 비율
- **"AI 학습이 거창하지 않을 수 있다"**는 감각

---

## 3. 다음 토큰 예측: 문장 생성 체험

### 실습 3-1: 프롬프트로 생성

```bash
# "오늘은"으로 시작하는 문장 생성
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --seed 42"
```

**예상 결과**:
```
오늘은 날씨가 좋다 ...
또는
오늘은 기분이 좋다 ...
```

**관찰 포인트**:
1. "오늘은" 다음에 자주 나왔던 단어가 선택됨
2. 선택된 단어를 붙이고, 그 단어를 기준으로 다시 다음 토큰 선택
3. 이 과정을 maxTokens만큼 반복

**교육 포인트**:
- AI는 문장을 "한 번에 만들지 않아요"
- **"토큰 하나씩 골라서 붙이는 반복"**이에요

### 실습 3-2: Seed로 재현성 확인

```bash
# 같은 seed = 같은 결과
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --seed 42"
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --seed 42"

# 다른 seed = 다른 결과
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --seed 123"
```

**교육 포인트**:
- **확률적 선택**: 여러 후보 중 하나를 랜덤하게 고름
- seed를 고정하면 같은 선택을 재현 가능
- 이것이 **"같은 질문에 다른 답이 나오는 이유"**

---

## 4. Usage 측정: 토큰 = 비용

### 실습 4-1: 토큰 수에 따른 Usage 변화

```bash
# 짧은 프롬프트
./gradlew :mini-ai-cli:run --args="run -p '좋다' --max-tokens 5"

# 긴 프롬프트
./gradlew :mini-ai-cli:run --args="run -p '오늘은 날씨가 좋다' --max-tokens 5"
```

**관찰 포인트**:
```
Usage:
  Input:  1 tokens  (프롬프트: "좋다")
  Output: 5 tokens
  Total:  6 tokens

vs

Usage:
  Input:  4 tokens  (프롬프트: "오늘은 날씨가 좋다")
  Output: 5 tokens
  Total:  9 tokens
```

**교육 포인트**:
- Input tokens = 프롬프트 토큰 수
- Output tokens = 생성된 토큰 수
- **Total tokens = 비용의 기준**
- 같은 output이어도 input이 길면 total이 증가

### 실습 4-2: MaxTokens와 비용

```bash
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 5"
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 20"
```

**교육 포인트**:
- maxTokens를 크게 하면 Output tokens 증가
- **더 긴 답변 = 더 많은 토큰 = 더 높은 비용**

---

## 5. Temperature와 TopK: 선택의 다양성

### 실습 5-1: Temperature 비교

```bash
# 낮은 temperature = 안정적, 예측 가능
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --temperature 0.1 --seed 42"

# 높은 temperature = 창의적, 다양함
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --temperature 2.0 --seed 42"
```

**교육 포인트**:
- Temperature 낮음: 확률 높은 후보만 주로 선택
- Temperature 높음: 확률 낮은 후보도 선택 가능
- **"답변의 창의성 vs 안정성" 조절**

### 실습 5-2: TopK 비교

```bash
# TopK=1 = 항상 1등만 선택
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --top-k 1"

# TopK=50 = 상위 50개 중 선택
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --top-k 50"
```

**교육 포인트**:
- TopK: 상위 K개 후보로 제한
- **선택지를 제한해서 품질 vs 다양성 조절**

---

## 6. Bigram의 한계: 짧은 문맥

### 실습 6-1: 문맥이 짧아서 생기는 문제

```bash
# Bigram은 직전 1개만 본다
./gradlew :mini-ai-cli:run --args="run -p '좋다' --max-tokens 20 --seed 42"
```

**예상 결과**:
```
좋다 → 기분이 → 좋다 → 기분이 → 좋다 ... (반복)
```

**왜 반복될까?**
```
Bigram 관점:
"좋다" → 다음 후보: "기분이", "날씨가", ...
"기분이" → 다음 후보: "좋다", "나쁘다", ...
"좋다" → 다음 후보: "기분이", ... (다시 반복)
```

**교육 포인트**:
- Bigram은 **직전 1개만** 보기 때문에 문맥을 잃기 쉬움
- 같은 패턴에 빠지면 벗어나기 어려움
- **이것이 "더 긴 문맥이 필요한 이유"**

### 실습 6-2: Trigram이라면?

**Trigram 관점 (가상)**:
```
"오늘은" + "날씨가" → "좋다"
"날씨가" + "좋다" → "산책을"
"좋다" + "산책을" → "간다"
```

**차이점**:
- 직전 2개를 보면 **문맥이 더 안정됨**
- "좋다" 단독이 아니라 "날씨가 좋다"라는 흐름을 기억
- **한국어의 조사/어미 패턴을 더 잘 포착**

---

## 7. 희소성 문제: "처음 보는 문맥"

### 실습 7-1: 학습에 없던 단어로 시작

```bash
# 학습 데이터에 없는 단어
./gradlew :mini-ai-cli:run --args="run -p '컴퓨터' --max-tokens 10"
```

**예상 결과**:
```
Error 또는 [UNK] 토큰 사용
```

**왜?**
```
Bigram 표:
  "오늘은" → {...}
  "날씨가" → {...}
  "기분이" → {...}
  "컴퓨터" → ???  (표에 없음!)
```

**교육 포인트**:
- **표에 없는 문맥 = 예측 불가**
- 이것이 "희소성(sparsity)" 문제
- Trigram은 이 문제가 더 심함 (조합이 기하급수적으로 증가)

### 실습 7-2: 해결 방법 (개념)

문서에서 설명한 세 가지 방법:

1. **Smoothing (스무딩)**
   - 표에 없어도 아주 작은 확률 부여
   - "완전히 불가능은 아니다"

2. **Backoff (백오프)**
   - Trigram 없으면 → Bigram으로
   - Bigram 없으면 → Unigram으로
   - **문맥을 점점 짧게 보면서 fallback**

3. **Interpolation (보간)**
   - Trigram + Bigram + Unigram을 항상 섞어서 사용
   - 예: `P = 0.5*P3 + 0.3*P2 + 0.2*P1`
   - **안정성과 문맥성의 균형**

---

## 8. 최신 AI와의 연결

### 질문: "그럼 GPT도 n-gram인가요?"

**답**: 고정된 n-gram은 아니지만, 겉모습은 비슷해요.

| 비교 항목 | Bigram | 최신 AI (GPT/Claude) |
|----------|--------|---------------------|
| 문맥 길이 | 직전 1개 | 수천~수만 개 토큰 |
| 확률 계산 | 표(카운트) | 뉴럴 네트워크 계산 |
| 학습 방식 | 세기 | 경사하강법 |
| 처리 단위 | 토큰 | 토큰 |
| 생성 방식 | 다음 토큰 하나씩 | 다음 토큰 하나씩 |

**공통점**:
- 둘 다 **"다음 토큰을 예측"**
- 둘 다 **토큰 단위로 처리**
- 둘 다 **반복하면서 문장 생성**

**차이점**:
- Bigram: "외운 표"
- 최신 AI: "계산으로 만든 확률"

---

## 9. 실전 응용: 이 프로젝트로 이해할 수 있는 것들

### 9-1. 왜 대화가 길어지면 앞을 잊을까?

```bash
# 짧은 대화
Input: 10 tokens → 처리 가능

# 긴 대화
Input: 10000 tokens → 앞부분이 범위 밖으로
```

**이유**: 참고할 수 있는 **토큰 길이에 한계**

### 9-2. 왜 비용이 토큰 기준일까?

```
Input tokens = 읽어야 할 조각
Output tokens = 만들어낸 조각
Total = 처리한 전체 조각 수

처리량 ∝ 토큰 수 ∝ 비용
```

### 9-3. 왜 결과가 매번 달라질까?

```
다음 토큰 선택 시:
  후보1: 50% 확률
  후보2: 30% 확률
  후보3: 20% 확률

→ 확률적으로 선택
→ 매번 다른 후보가 나올 수 있음
```

### 9-4. 왜 특수문자/이모지가 비쌀까?

```
"안녕하세요" = 1 token
"안녕하세요😊" = 2 tokens (텍스트 + 이모지)
"v1.2.3" = 여러 tokens ("v", "1", ".", "2", ".", "3")

→ 토큰화기가 자주 못 본 패턴을 잘게 쪼갬
→ 토큰 수 증가 → 비용 증가
```

---

## 10. 마무리: 이 실습으로 얻을 수 있는 것

✅ **토큰이 AI의 기본 단위**라는 직관
✅ **다음 토큰 예측이 핵심 동작**이라는 이해
✅ **비용/속도/기억이 모두 토큰과 연결**된다는 감각
✅ **Bigram → Trigram → 최신 AI**로 이어지는 자연스러운 흐름
✅ **희소성 문제와 해결 방향**(smoothing, backoff, interpolation)

---

## 다음 단계

이 실습을 마치면 다음 내용을 자연스럽게 학습할 수 있어요:

1. **Trigram 구현** (Step 7의 확장 로드맵)
2. **Backoff/Smoothing 구현**
3. **더 긴 문맥 처리** (Transformer 입문)
4. **실전 토크나이저** (BPE, WordPiece, SentencePiece)

---

**이제 "토큰"은 더 이상 신비로운 개념이 아니라, 손으로 만질 수 있는 구체적인 개념이 되었습니다!** 🎉
