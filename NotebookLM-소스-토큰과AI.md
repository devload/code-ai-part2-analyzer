# 토큰으로 이해하는 AI의 비밀
## From Bigram to Modern AI - 실습으로 배우는 토큰과 N-gram

> **발표 목적**: AI가 "신기한 상자"에서 "이해 가능한 시스템"으로 바뀌는 순간을 경험하게 하기
>
> **대상**: AI를 사용해봤지만 내부 동작이 궁금한 개발자, 학생, 기획자
>
> **핵심 메시지**: 토큰을 이해하면 AI의 모든 행동이 설명 가능해진다

---

## 🎯 발표 개요 (Opening)

### 우리가 겪는 AI의 신비로운 현상들

1. **왜 같은 질문인데 답이 매번 조금씩 다를까?**
2. **대화가 길어지면 왜 앞 이야기를 잊는 것처럼 보일까?**
3. **왜 비용이 "글자 수"가 아니라 "토큰 수"로 계산될까?**
4. **토큰은 도대체 어디서 끊기고, 모델마다 왜 다를까?**

### 이 발표가 끝나면

- ✅ 토큰이 무엇인지 손으로 만질 수 있게 됩니다
- ✅ AI가 문장을 만드는 과정을 직접 실행해봅니다
- ✅ Bigram에서 시작해서 GPT까지 자연스럽게 연결됩니다
- ✅ 비용, 속도, 품질의 관계를 명확히 이해합니다

---

## Part 1: 토큰이란 무엇인가?

### 1-1. AI는 문장을 그대로 읽지 않는다

**핵심 개념**:
```
사람: "오늘은 날씨가 좋다" (통째로 이해)
AI: ["오늘은", "날씨가", "좋다"] (조각으로 분해)
```

**왜 자를까?**
- 컴퓨터는 숫자를 다룬다
- 텍스트를 "숫자로 풀 수 있는 문제"로 바꿔야 한다
- 자른 조각 = **토큰**

### 1-2. 토큰화 실제 체험

**실습 예제**:
```bash
입력: "오늘은 날씨가 좋다"
출력: ["오늘은", "날씨가", "좋다"] = 3개 토큰

입력: "할 수 있다"
출력: ["할", "수", "있다"] = 3개 토큰

입력: "할수있다"
출력: ["할수있다"] = 1개 토큰
```

**💡 핵심 인사이트**:
- 띄어쓰기에 따라 토큰 수가 달라진다
- 토큰 수 = 비용의 기준
- **"할 수 있다" = 비용 3배, "할수있다" = 비용 1배**

### 1-3. 토큰의 크기는 모델마다 다르다

**토큰의 종류**:
- 글자 단위: "오" + "늘" + "은" = 3토큰
- 단어 단위: "오늘은" = 1토큰
- 서브워드 (최신 AI): "오늘" + "은" = 2토큰

**왜 모델마다 다를까?**
- 처음 보는 단어를 어떻게 다룰지의 전략 차이
- 효율성과 일반화의 균형

---

## Part 2: AI는 토큰으로 무엇을 하는가?

### 2-1. 핵심 동작: "다음 토큰 맞히기"

**AI의 본질**:
```
지금까지: ["오늘은", "날씨가"]
다음은?: "좋다" (예측)
```

**문장 생성 과정**:
1. 지금까지 나온 토큰들을 본다
2. 다음 토큰 하나를 고른다
3. 그것을 붙인다
4. 다시 2번으로 (반복)

**💡 핵심 인사이트**:
- AI는 문장을 "한 번에 만들지 않는다"
- **토큰 하나씩 골라서 붙이는 반복**
- 우리가 보는 유창한 문장 = 이 반복의 결과

### 2-2. Bigram: 가장 단순한 "다음 토큰 맞히기"

**Bigram의 규칙**:
```
직전 1개만 보고, 다음 1개를 예측
```

**학습 방식**:
```
데이터: "오늘은 날씨가 좋다"
       "오늘은 기분이 좋다"

학습 결과 (카운트 테이블):
  오늘은 → 날씨가: 1번
  오늘은 → 기분이: 1번
  날씨가 → 좋다: 1번
  기분이 → 좋다: 1번
```

**💡 핵심 인사이트**:
- 학습 = "자주 붙는 쌍을 세기"
- **거창한 수학이 아니라 카운팅**
- 확률 = 카운트 비율

### 2-3. Bigram으로 문장 생성하기

**생성 과정**:
```
1. 시작: "오늘은"
2. 표 확인: "오늘은" → 날씨가(50%), 기분이(50%)
3. 하나 선택: "날씨가" (랜덤 또는 확률적)
4. 붙이기: "오늘은 날씨가"
5. 반복: "날씨가" → 좋다(100%)
6. 결과: "오늘은 날씨가 좋다"
```

**💡 핵심 인사이트**:
- 확률적으로 고르기 때문에 **매번 결과가 다를 수 있다**
- 이것이 "같은 질문에 다른 답"의 원리

---

## Part 3: 실전 프로젝트 - Mini AI 구현

### 3-1. 프로젝트 구조

```
Mini AI Full-Stack (Java)
│
├── mini-ai-core
│   └── 인터페이스 정의 (Tokenizer, LanguageModel, Trainer)
│
├── mini-ai-tokenizer-simple
│   └── WhitespaceTokenizer (띄어쓰기 기준 토큰화)
│
├── mini-ai-model-ngram
│   ├── BigramTrainer (학습 = 카운팅)
│   ├── BigramModel (생성 = 다음 토큰 선택)
│   └── Sampler (topK, temperature)
│
├── mini-ai-server
│   └── REST API (Spring Boot)
│       ├── POST /v1/train
│       └── POST /v1/generate
│
└── mini-ai-cli
    └── 명령줄 도구
        ├── train
        ├── run
        ├── chat
        └── tokenize
```

### 3-2. Step별 학습 여정

**Step 0: 뼈대 만들기**
- 교체 가능한 구조 설계
- 인터페이스 우선 설계

**Step 1: Tokenizer**
- 텍스트 → 토큰 변환
- encode/decode 구현

**Step 2: Bigram 학습**
- 코퍼스 → 카운트 테이블
- JSON artifact 저장

**Step 3: Bigram 생성**
- 다음 토큰 예측 루프
- topK, temperature 샘플링

**Step 4: Usage 측정**
- input/output/total 토큰 추적
- **토큰 = 비용 단위**

**Step 5: Server**
- REST API로 서빙
- latency 측정

**Step 6: CLI**
- 사용자 친화적 인터페이스
- REPL 대화 모드

**Step 7: 확장 설계**
- Trigram 확장 포인트
- Backoff, Interpolation

### 3-3. 실제 코드 핵심

**BigramTrainer.java - 학습의 본질**:
```java
// 학습 = 카운팅
for (int i = 0; i < tokens.size() - 1; i++) {
    int prev = tokens.get(i);
    int next = tokens.get(i + 1);
    counts[prev][next]++;  // 자주 붙는 쌍을 센다!
}
```

**BigramModel.java - 생성의 본질**:
```java
// 생성 = 다음 토큰을 반복해서 고르기
for (int i = 0; i < maxTokens; i++) {
    int prevToken = currentTokens.get(currentTokens.size() - 1);
    Map<Integer, Integer> nextCounts = artifact.getNextTokenCounts(prevToken);
    int nextToken = sampler.sample(nextCounts);  // 확률적 선택
    currentTokens.add(nextToken);
}
```

**Sampler.java - 확률적 선택**:
```java
// Temperature: 창의성 vs 안정성
// topK: 선택지 제한

// Temperature 낮음 (0.1) = 안정적, 예측 가능
// Temperature 높음 (2.0) = 창의적, 다양함
```

---

## Part 4: 토큰으로 설명되는 AI의 모든 현상

### 4-1. 비용: 왜 토큰 수가 기준인가?

**비용 구조**:
```
Input tokens:  프롬프트를 읽는 비용
Output tokens: 답변을 만드는 비용
Total tokens:  실제 청구 기준
```

**실습 결과**:
```
짧은 프롬프트:
  Input: 1 token ("좋다")
  Output: 5 tokens
  Total: 6 tokens

긴 프롬프트:
  Input: 4 tokens ("오늘은 날씨가 좋다")
  Output: 5 tokens
  Total: 9 tokens = 비용 50% 증가!
```

**💡 핵심 인사이트**:
- 같은 답변이어도 프롬프트가 길면 비용 증가
- maxTokens를 크게 하면 비용 증가
- **토큰 = 계산 단위 = 비용 단위**

### 4-2. 속도: 왜 토큰이 많으면 느려질까?

**처리 과정**:
```
Input tokens:  읽어야 할 조각 수
Output tokens: 만들어야 할 조각 수
→ 토큰 많을수록 = 처리량 증가 = 속도 감소
```

**Bigram의 경우**:
- 다음 토큰을 고르려면 "지금 토큰"을 보고 표 검색
- 토큰이 많으면 = 반복 횟수 증가

**최신 AI의 경우**:
- 다음 토큰을 고르려면 "지금까지 모든 토큰" 계산
- 토큰이 많으면 = 계산량 폭발적 증가

### 4-3. 기억: 왜 대화가 길어지면 잊을까?

**참고 범위의 한계**:
```
Bigram:   직전 1개만 참고
Trigram:  직전 2개만 참고
GPT-4:    약 128,000 토큰까지 참고 (하지만 무한은 아님)
```

**긴 대화의 문제**:
```
대화가 10,000 토큰을 넘어가면:
  - 앞부분이 참고 범위 밖으로 밀려남
  - 또는 영향력이 약해짐

→ "아까 말한 거 잊었네?"
```

**💡 핵심 인사이트**:
- AI의 기억 = 토큰으로 이루어짐
- **기억 길이에는 한계가 있다**

### 4-4. 다양성: 왜 매번 답이 다를까?

**확률적 선택의 원리**:
```
"오늘은" 다음 후보:
  - 날씨가: 50% 확률
  - 기분이: 50% 확률

선택 1: "날씨가" 선택 → "오늘은 날씨가 좋다"
선택 2: "기분이" 선택 → "오늘은 기분이 좋다"
```

**Temperature의 역할**:
```
Low (0.1):  거의 항상 1등 선택 → 안정적
High (2.0): 낮은 확률도 선택 가능 → 창의적
```

**💡 핵심 인사이트**:
- AI는 "정답을 꺼내는" 방식이 아님
- **"확률적으로 선택하는" 방식**
- 이것이 다양성과 창의성의 원천

---

## Part 5: Bigram의 한계와 Trigram

### 5-1. Bigram의 근본적 문제

**짧은 문맥의 한계**:
```
Bigram: "좋다" → 다음은?
→ 앞 문맥을 모르니 "날씨가"인지 "기분이"인지 모호

문제: 반복에 빠지기 쉬움
"좋다" → "기분이" → "좋다" → "기분이" → ...
```

**실제 생성 결과**:
```
프롬프트: "좋다"
생성: "좋다 기분이 좋다 기분이 좋다 기분이 ..."
```

### 5-2. Trigram: 문맥을 늘리면

**Trigram의 규칙**:
```
직전 2개를 보고, 다음 1개를 예측
```

**차이점**:
```
Bigram:  "좋다" → ?
Trigram: "날씨가 좋다" → ?

→ 문맥이 더 명확해짐
```

**한국어에서의 중요성**:
```
"할 수" → "있다" (O)
"하기 위해" → "노력한다" (O)
"되고" → "있다" (O)

→ 조사/어미는 최소 2토큰 이상이 함께 움직임
→ Trigram부터 자연스러워짐
```

### 5-3. Trigram의 새로운 문제: 희소성

**희소성 (Sparsity) 문제**:
```
Bigram:  조합 수 = V × V
Trigram: 조합 수 = V × V × V

V = 10,000일 때:
  Bigram:  1억 가지 조합
  Trigram: 1조 가지 조합!

→ "처음 보는 조합"이 폭발적으로 증가
```

**표에 없는 문맥**:
```
학습 데이터에 ("컴퓨터", "프로그래밍") 조합이 없다면?
→ 다음 토큰을 예측할 수 없음
→ 생성이 멈추거나 이상해짐
```

### 5-4. 희소성 해결 방법

**1. Smoothing (스무딩)**:
```
표에 없어도 아주 작은 확률 부여
"완전 불가능은 아니다"
```

**2. Backoff (백오프)**:
```
Trigram 없으면 → Bigram으로
Bigram 없으면 → Unigram으로
더 짧은 문맥으로 fallback
```

**3. Interpolation (보간)**:
```
항상 섞어서 사용:
P = 0.5 × P_trigram + 0.3 × P_bigram + 0.2 × P_unigram

→ 안정성과 문맥성의 균형
```

---

## Part 6: Bigram → GPT로의 자연스러운 확장

### 6-1. GPT는 몇-gram인가?

**질문**: "GPT, Claude, Gemini는 몇-gram인가요?"

**답변**:
```
고정된 n-gram이 아님
하지만 겉모습은 "n이 매우 큰 n-gram"처럼 동작
```

**비교표**:

| 항목 | Bigram | 최신 AI (GPT/Claude) |
|------|--------|---------------------|
| 문맥 길이 | 직전 1개 | 수천~수만 개 토큰 |
| 확률 계산 | 표 (카운트) | 뉴럴 네트워크 |
| 학습 방식 | 세기 | 경사하강법 |
| 처리 단위 | 토큰 | 토큰 |
| 생성 방식 | 다음 토큰 하나씩 | 다음 토큰 하나씩 |
| "처음 보는 문맥" | 멈춤/오류 | 일반화로 처리 |

### 6-2. 공통점: 본질은 같다

**둘 다**:
1. 텍스트를 **토큰**으로 자른다
2. 지금까지를 보고 **다음 토큰을 예측**한다
3. 하나씩 골라서 **반복**하면서 문장을 만든다

**차이점**:
- **Bigram**: "외운 표"에서 꺼냄
- **최신 AI**: "계산"으로 만듦

### 6-3. 왜 Bigram을 배워야 하는가?

**이유 1: 핵심 구조가 보인다**
- 최신 AI도 "다음 토큰 예측의 반복"
- 복잡한 수학에 가려진 본질을 Bigram이 보여줌

**이유 2: 문제와 해결책이 명확하다**
- 희소성 문제 → Smoothing, Backoff
- 짧은 문맥 → Trigram, 더 긴 n
- 이 문제들이 왜 Transformer가 나왔는지 설명

**이유 3: 직접 만들 수 있다**
- 학습 = 카운팅 (누구나 이해 가능)
- 생성 = 표 보고 선택 (명확함)
- **"AI를 만든다"는 경험**

---

## Part 7: 실습으로 체험하기

### 7-1. 빠른 시작 (5분)

**실행 방법**:
```bash
./examples/빠른시작-토큰체험.sh
```

**자동으로 시연되는 내용**:
1. ✅ 토큰화 - "오늘은 날씨가 좋다" → 3개 토큰
2. ✅ 토큰 수 비교 - 띄어쓰기의 영향
3. ✅ Bigram 학습 - 자주 붙는 쌍 세기
4. ✅ 문장 생성 - 다음 토큰 예측 반복
5. ✅ Seed와 재현성 - 확률적 선택
6. ✅ Usage 측정 - 토큰 = 비용
7. ✅ Temperature - 창의성 조절

### 7-2. 상세 실습 (30분)

**가이드 문서**:
```bash
cat examples/토큰-개념-실습.md
```

**실습 내용**:

**실습 1: 토큰화**
```bash
./gradlew :mini-ai-cli:run --args="tokenize '할 수 있다'"
→ [할, 수, 있다] = 3토큰

./gradlew :mini-ai-cli:run --args="tokenize '할수있다'"
→ [할수있다] = 1토큰

💡 비용이 3배 차이!
```

**실습 2: Bigram 학습**
```bash
./gradlew :mini-ai-cli:run --args="train --corpus data/교육용-토큰개념.txt --output data/교육-bigram.json"

→ Vocabulary: 200+
→ Total bigrams: 300+
→ 학습 완료!
```

**실습 3: 문장 생성**
```bash
./gradlew :mini-ai-cli:run --args="run -p '오늘은' --max-tokens 10 --seed 42"

→ "오늘은 날씨가 좋다 산책을 간다 ..."
```

**실습 4: 확률적 선택**
```bash
# 같은 seed = 같은 결과
./gradlew :mini-ai-cli:run --args="run -p '날씨가' --max-tokens 5 --seed 42"
./gradlew :mini-ai-cli:run --args="run -p '날씨가' --max-tokens 5 --seed 42"
→ 결과 동일!

# 다른 seed = 다른 결과
./gradlew :mini-ai-cli:run --args="run -p '날씨가' --max-tokens 5 --seed 123"
→ 결과 다름!
```

**실습 5: Usage 비교**
```bash
짧은 프롬프트:
  Input: 1 token
  Output: 5 tokens
  Total: 6 tokens

긴 프롬프트:
  Input: 5 tokens
  Output: 5 tokens
  Total: 10 tokens
```

**실습 6: Temperature 효과**
```bash
Low (0.1):  안정적, 예측 가능한 문장
High (2.0): 창의적, 다양한 문장
```

### 7-3. 교육용 데이터셋

**data/교육용-토큰개념.txt**:
- Bigram의 확률적 선택 체험
- "오늘은" → 날씨가 / 기분이 (50% vs 50%)
- 다중 경로 문장 생성

**data/교육용-문맥차이.txt**:
- 한국어 조사/어미 패턴
- "할 수 있다", "하고 싶다", "되고 있다"
- Trigram 필요성 체험

---

## Part 8: 핵심 Takeaways

### 8-1. 3가지 핵심 개념

**1. 토큰 = AI의 기본 단위**
```
- 읽는 단위
- 쓰는 단위
- 기억하는 단위
- 비용 계산 단위
```

**2. 다음 토큰 예측 = AI의 핵심 동작**
```
- 학습: 자주 붙는 패턴 학습
- 생성: 다음 토큰 하나씩 예측
- 반복: 문장 완성까지
```

**3. 문맥 길이 = 품질의 핵심**
```
- Bigram (1개):  짧은 문맥, 반복 쉬움
- Trigram (2개): 자연스러워짐, 희소성 증가
- 최신 AI (수만 개): 매우 자연스러움, 비용 증가
```

### 8-2. AI를 이해하는 새로운 관점

**Before**:
- AI = 신기한 상자
- 어떻게 동작하는지 모름
- 비용이 왜 이렇게 나오는지 모름

**After**:
- AI = 토큰 예측 시스템
- 학습 = 패턴 카운팅 (또는 계산)
- 생성 = 다음 토큰 반복 선택
- **모든 현상이 토큰으로 설명 가능**

### 8-3. 실전 응용

**프롬프트 최적화**:
```
- 불필요한 단어 제거 = 토큰 수 감소 = 비용 감소
- 핵심만 명확히 = 효율적
```

**대화 관리**:
```
- 긴 대화는 주기적으로 요약
- 중요한 정보는 최근에 배치
- 참고 범위 한계 고려
```

**품질 vs 비용**:
```
- Temperature 조절로 창의성 제어
- maxTokens로 답변 길이 제한
- 토큰 수 모니터링
```

---

## Part 9: 다음 단계로

### 9-1. 이 프로젝트의 확장 로드맵

**Phase 1: Trigram 구현**
- TrigramTrainer
- TrigramModel
- 더 자연스러운 문장

**Phase 2: 희소성 처리**
- Smoothing 구현
- Backoff 구현
- Interpolation 구현

**Phase 3: 고급 기능**
- Variable-length N-gram
- Neural N-gram
- Attention 메커니즘 입문

### 9-2. 더 깊이 공부하기

**토크나이저**:
- BPE (Byte Pair Encoding)
- WordPiece
- SentencePiece

**언어 모델**:
- LSTM
- Transformer
- GPT 구조

**실전 응용**:
- RAG (Retrieval-Augmented Generation)
- Fine-tuning
- Prompt Engineering

### 9-3. 이 프로젝트의 가치

**교육적 가치**:
- ✅ 이론과 실습의 완벽한 통합
- ✅ 단계별 점진적 학습
- ✅ 직접 만들어보는 경험
- ✅ 최신 AI로의 자연스러운 연결

**실용적 가치**:
- ✅ 토큰 개념 확실히 이해
- ✅ 비용 최적화 가능
- ✅ 프롬프트 엔지니어링 기초
- ✅ AI 시스템 설계 감각

---

## Closing: AI는 더 이상 신기한 상자가 아니다

### 우리가 배운 것

**토큰**:
- AI가 보는 세상의 조각
- 비용, 속도, 기억의 기준

**다음 토큰 예측**:
- AI의 핵심 동작 원리
- Bigram도, GPT도 본질은 같음

**문맥**:
- 짧으면 단순, 길면 자연스러움
- 하지만 비용과 희소성의 트레이드오프

### 이제 할 수 있는 것

**1. AI의 행동을 설명할 수 있다**
- "왜 비쌀까?" → 토큰 수
- "왜 잊을까?" → 문맥 길이 한계
- "왜 다를까?" → 확률적 선택

**2. AI를 효율적으로 사용할 수 있다**
- 프롬프트 최적화
- 비용 관리
- 품질 제어

**3. 더 깊은 학습의 기반이 생겼다**
- Trigram, Transformer 등으로 확장 가능
- 원리를 알기에 응용이 쉬움

### 마지막 메시지

> **"AI를 이해한다는 것은 마법을 파괴하는 게 아니라,
> 더 정교하게 사용할 수 있는 힘을 얻는 것입니다."**

**감사합니다!** 🎉

---

## 부록: 프로젝트 정보

### 저장소 구조
```
mini-ai/
├── mini-ai-core/              # 인터페이스/DTO
├── mini-ai-tokenizer-simple/  # 토큰화
├── mini-ai-model-ngram/       # Bigram 모델
├── mini-ai-server/            # REST API
├── mini-ai-cli/               # CLI 도구
├── data/                      # 코퍼스 & Artifact
├── examples/                  # 실습 가이드
└── docs/                      # Step별 문서
```

### 기술 스택
- Java 17
- Gradle 8.5
- Spring Boot 3.2.0
- picocli 4.7.5
- JUnit 5

### Git 태그
- step-00 ~ step-07: 각 단계별 완성 상태
- 순차적 학습 가능

### 참고 자료
- README.md: 프로젝트 개요
- examples/토큰-개념-실습.md: 상세 실습 가이드
- docs/STEP-XX.md: 각 단계별 설명
- ../doc/토큰을 모르면 AI는 늘 신기한 상자로 남아요.md: 이론 문서

---

**이 문서는 NotebookLM으로 PPT를 생성하기 위한 소스입니다.**
**발표 구조, 핵심 내용, 실습 예제, 시각화 요소가 모두 포함되어 있습니다.**
